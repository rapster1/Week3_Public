{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "David's multiclass of Mammogram_Analysis_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rapster1/Week3_Public/blob/master/ChallengeProject/FinalMulticlassificationModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9iIELyqJo0T",
        "colab_type": "text"
      },
      "source": [
        "# Mammogram Analysis Challenge\n",
        "\n",
        "A mammogram is an X-ray of the breast used primarily by radiologists to detect signs of breast cancer like abnormal masses and calcifications (signs of a possible tumor). Sometimes these abnormalities are obvious, sometimes they are not. In younger women especially, masses and calcifications can be difficult for radiologists to detect in a mammogram due to dense surrounding tissue. This makes mammograms a great candidate for machine learning analysis. A program that can detect abnormalities with high accuracy could be used to catch radiologists' mistakes and potentially save lives (in fact, it already isâ€”https://giving.massgeneral.org/machine-learning-breast-cancer-screening)\n",
        "\n",
        "\n",
        "  <h3><center>Mammogram Image</center></h3>\n",
        "\n",
        "<center><img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week3/mammogram_image.png\" width=400></center>\n",
        "\n",
        "\n",
        "For this challenge you will classify mammogram segments into two classes (positive for an abnormality and negative). You may also attempt to use the \"multiclass labels\" to classify the mammograms into five classes (negative, positive benign calcification, positive malignant calcification, positive benign mass, and positive malignant mass). We recommend you begin with the binary labels, but try both if you have time!\n",
        "\n",
        "Below we provide you with code to visualize the dataset as well as a simple but functional model that does a relatively poor job at this classification problem. It's up to you to build a classifier that does better! Your models will be evaluated based on confusion matrix , Area under the ROC Curve, and Creativity.\n",
        "\n",
        "Before you run any code make sure you turn GPU support on! (Edit > Notebook Settings > Hardware Accelerator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ukBWGU0O9l",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNtIxtyy5qS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run this cell to import the packages you will need to unpack the dataset\n",
        "# File manipulation and IO (input/output)\n",
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Import numerical and dataframe handling\n",
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "\n",
        "# Data preprocessing\n",
        "from PIL import Image\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Model scoring\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "# Import standard machine learning machinery\n",
        "import tensorflow as tf\n",
        "\n",
        "# Garbage collection (for saving RAM during training)\n",
        "import gc\n",
        "\n",
        "# Import plotting functionality\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "import matplotlib\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbWAIUdOrF01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set plotting preferences\n",
        "%matplotlib inline\n",
        "font = {'family' : 'sans-serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 16}\n",
        "matplotlib.rc('font', **font)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqfIbs4Jy8nH",
        "colab_type": "text"
      },
      "source": [
        "## Download the images and labels from our GitHub data repository\n",
        "\n",
        "This dataset is from the publicly available Digital Database for Screening Mammography (DDSM) and the Curated Breast Imaging Subset of DDSM (CBIS-DDSM).\n",
        "\n",
        "The binary labels are 0 (negative for an abnormality) and 1 (positive for an abnormality). The multiclass labels are:\n",
        "* 0 = Negative\n",
        "* 1 = Benign Calcification\n",
        "* 2 = Benign Mass\n",
        "* 3 = Malignant Calcification\n",
        "* 4 = Malignant Mass\n",
        "\n",
        "In the following cell, we download the images and labels from our GitHub data repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkpOzgA9w7jK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "87f3bd95-de3f-48a2-8018-75df0842110a"
      },
      "source": [
        "# Set the path to the Week 3 challenge data on GitHub\n",
        "github_data_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/ChallengeProjects/Week3/'\n",
        "\n",
        "# Download 3 zip files from github containing training, validation, and test images\n",
        "os.system('wget '+ os.path.join(github_data_path, 'Mamm_Images_Test.zip'))\n",
        "os.system('wget '+ os.path.join(github_data_path, 'Mamm_Images_Train.zip'))\n",
        "os.system('wget '+ os.path.join(github_data_path, 'Mamm_Images_Val.zip'))\n",
        "\n",
        "# Download the binary and multiclass labels for the \n",
        "os.system('wget ' + os.path.join(github_data_path,'train_binary_labels.csv'))\n",
        "os.system('wget ' + os.path.join(github_data_path,'train_multiclass_labels.csv'))\n",
        "os.system('wget ' + os.path.join(github_data_path,'val_binary_labels.csv'))\n",
        "os.system('wget ' + os.path.join(github_data_path,'val_multiclass_labels.csv'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWwqOpa9Kroz",
        "colab_type": "text"
      },
      "source": [
        "## Loading the data into memory\n",
        "\n",
        "Run the cell below to get all the data for this challenge. After running the cell, the following data will be loaded into the following variables:\n",
        "\n",
        "* **train_binary_labels_df**: 2 column pandas df where each row contains a label (0 or 1) and the unique ID of the image the label corresponds to. 5500 samples/rows.\n",
        "* **train_multiclass_labels_df**: 2 column pandas df where the rows contains a label (0, 1, 2, 3, 4) and the unique ID of the image the label corresponds to. 5500 samples/rows.\n",
        "* **train_images_df**: 2 column pandas df where the rows contain a 299x299 numpy array of the (greyscale) pixel values in the image and the unique ID of the image. 5500 samples/rows. \n",
        "* **train_images**: A np array (5500, 299,299) containing all training images (may be easier to deal with than the df)\n",
        "* **test_images_df**: 2 column pandas df where the rows contain a 299x299 numpy array of the (greyscale) pixel values in the image and the unique ID of the image. 1500 samples/rows.\n",
        "* **test_images**: A np array (1500, 299,299) containing all training images (may be easier to deal with than the df)\n",
        "\n",
        "\n",
        "DO NOT USE THE TEST IMAGES IN MODEL DEVELOPMENT AT ALL. ALSO, DO NOT SHUFFLE THE TEST IMAGES. These images will be used to evaluate your model and should only be run through the classifier ONCE at the time of submission. You will not have access to the test image labels.\n",
        "\n",
        "Explore the variables listed above! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZektC25k3Jfp",
        "colab_type": "text"
      },
      "source": [
        "### Unzip the Image Files\n",
        "\n",
        "The following cell unzips the image files and prepares them to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZH9ZTL6y6NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Unzip the image data and populate dataframe objects ###\n",
        "# Unzip all files to respective folders in current directory\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Train.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Val.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Test.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()  \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRXbNS6a1ncH",
        "colab_type": "text"
      },
      "source": [
        "### Store Images and Labels in Numpy Arrays and Dataframes\n",
        "\n",
        "The following cell will load the images and labels into a pair of numpy ndarray and pandas dataframe objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvoo9zFr5rTW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "78b05cb5-fdc3-42b9-e71a-a2d529e25932"
      },
      "source": [
        "# Use pd.read_csv to open csv file contents to pandas dataframes\n",
        "train_binary_labels_df = pd.read_csv('train_binary_labels.csv', header=None)\n",
        "train_multiclass_labels_df = pd.read_csv('train_multiclass_labels.csv', header=None)\n",
        "val_binary_labels_df = pd.read_csv('val_binary_labels.csv', header=None)\n",
        "val_multiclass_labels_df = pd.read_csv('val_multiclass_labels.csv', header=None)\n",
        "\n",
        "# Concatenate train/validation labels into one set\n",
        "#(you may decide to separate later for your own train/val/mocktest split)\n",
        "train_binary_labels_df = pd.concat([train_binary_labels_df, val_binary_labels_df], axis=0, ignore_index = True)\n",
        "train_multiclass_labels_df = pd.concat([train_multiclass_labels_df, val_multiclass_labels_df], axis=0, ignore_index = True)\n",
        "\n",
        "# Add columns to train_binary_labels_df dataframe that contains unique\n",
        "# (original) indices of train data\n",
        "unique_indices_df = pd.DataFrame(list(range(5500)))\n",
        "train_binary_labels_df = pd.concat([train_binary_labels_df, unique_indices_df], axis=1)\n",
        "train_binary_labels_df.columns = ['Label', 'Unique_Index']\n",
        "train_multiclass_labels_df = pd.concat([train_multiclass_labels_df, unique_indices_df], axis=1)\n",
        "train_multiclass_labels_df.columns = ['Label', 'Unique_Index']\n",
        "\n",
        "# Load images from file and save to both\n",
        "# 1) dataframe objects\n",
        "# 2) numpy arrays of shape (num_examples, num_pixels_wide, num_pixels_high)\n",
        "\n",
        "# Read in train images (from both train and val directories) to ndarray\n",
        "train_images = np.zeros((5500,299,299), dtype=np.uint8)\n",
        "for ind in range(5000):\n",
        "  im = plt.imread('Mamm_Images_Train/image' + str(ind) + '.jpg')\n",
        "  train_images[ind, :, :] = im\n",
        "  \n",
        "for ind in range(500):\n",
        "  im = plt.imread('Mamm_Images_Val/image' + str(ind) + '.jpg')\n",
        "  train_images[ind + 5000, :, :] = im\n",
        "\n",
        "train_images = np.repeat(train_images[...,np.newaxis],3,-1)\n",
        "\n",
        "# Read in test images to ndarray\n",
        "test_images = np.zeros((1500,299,299), dtype=np.uint8)\n",
        "for ind in range(1500):\n",
        "  im = plt.imread('Mamm_Images_Test/image' + str(ind) + '.jpg')\n",
        "  test_images[ind, :, :] = im\n",
        "\n",
        "test_images = np.repeat(test_images[...,np.newaxis],3,-1)\n",
        "\n",
        "\n",
        "# Make dataframes that contain the same information as the ndarrays\n",
        "# The N-th row contains a 299x299 ndarray, and a unique index for that image\n",
        "train_images_df = pd.DataFrame([[train_images[i,:,:] for i in range(train_images.shape[0])]])\n",
        "train_images_df = train_images_df.transpose()\n",
        "unique_indices_df = pd.DataFrame(list(range(5500)), dtype = np.int)\n",
        "train_images_df = pd.concat([train_images_df, unique_indices_df], axis = 1)\n",
        "train_images_df.columns = ['Images', 'Unique_Index']\n",
        "\n",
        "test_images_df = pd.DataFrame([[test_images[i,:,:] for i in range(test_images.shape[0])]])\n",
        "test_images_df = test_images_df.transpose()\n",
        "unique_indices_df = pd.DataFrame(list(range(5500, 7000)), dtype = np.int)\n",
        "test_images_df = pd.concat([test_images_df, unique_indices_df], axis = 1)\n",
        "test_images_df.columns = ['Images', 'Unique_Index']\n",
        "'''\n",
        "#Print out some information about populated variables\n",
        "print(f'train_images.shape: {train_images.shape}\\n')\n",
        "\n",
        "print(f'test_images.shape: {test_images.shape}\\n')\n",
        "\n",
        "print('train_images_df.head():')\n",
        "print(train_images_df.head())\n",
        "print('\\n')\n",
        "\n",
        "print('test_images_df.head():')\n",
        "print(test_images_df.head())\n",
        "print('\\n')\n",
        "\n",
        "print('10 random rows of train_binary_labels_df:')\n",
        "print(train_binary_labels_df.loc[np.random.randint(0,train_binary_labels_df.shape[0], size = (10,))])\n",
        "print('\\n')\n",
        "\n",
        "print('10 random rows of train_multiclass_labels_df:')\n",
        "print(train_multiclass_labels_df.loc[np.random.randint(0,train_multiclass_labels_df.shape[0], size = (10,))])\n",
        "print('\\n')\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#Print out some information about populated variables\\nprint(f'train_images.shape: {train_images.shape}\\n')\\n\\nprint(f'test_images.shape: {test_images.shape}\\n')\\n\\nprint('train_images_df.head():')\\nprint(train_images_df.head())\\nprint('\\n')\\n\\nprint('test_images_df.head():')\\nprint(test_images_df.head())\\nprint('\\n')\\n\\nprint('10 random rows of train_binary_labels_df:')\\nprint(train_binary_labels_df.loc[np.random.randint(0,train_binary_labels_df.shape[0], size = (10,))])\\nprint('\\n')\\n\\nprint('10 random rows of train_multiclass_labels_df:')\\nprint(train_multiclass_labels_df.loc[np.random.randint(0,train_multiclass_labels_df.shape[0], size = (10,))])\\nprint('\\n')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w22Vc8Z1qbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def resize (images_full_res, shape = (32,32), resize_images_bool=True):\n",
        "  # Specify a new shape to use for the resized images\n",
        "  # NOTE: For the InceptionV3 model, we must use a size of at least (75, 75).\n",
        "  # Other models may allow smaller input images.\n",
        "  original_shape = images_full_res.shape\n",
        "  new_shape = list(original_shape)\n",
        "  new_shape[1:3] = shape # put any image size you want!\n",
        "\n",
        "  # Compute if we are downsampling (in which case we need anti-aliasing)\n",
        "  scaling_ratio = np.array(new_shape[1:3])/np.array(original_shape[1:3])\n",
        "  anti_alias = np.any(scaling_ratio < 1)\n",
        "\n",
        "  # If resizing is requested, then run the resizing transformation\n",
        "  if resize_images_bool:\n",
        "    # Grab the original shape of the images\n",
        "    num_images = images_full_res.shape[0]\n",
        "\n",
        "    # Initialize an array for storing the resized images\n",
        "    images = np.zeros(new_shape, dtype=np.float16)\n",
        "\n",
        "    # Loop over each image in the data and perform a resizing operation\n",
        "    for img_num, img_data in enumerate(images_full_res):\n",
        "        # Update the user on progress\n",
        "        if np.mod(img_num, 1000) == 0:\n",
        "            print(f'Processing image number {img_num}')\n",
        "\n",
        "        # Process the image and force it to be a 16-bit float\n",
        "        processed_img = transform.resize(img_data, new_shape[1:],\n",
        "                                         anti_aliasing=anti_alias)\n",
        "        images[img_num] = processed_img.astype(np.float16)\n",
        "\n",
        "  # If no resizing requested, then just rename that data\n",
        "  else:\n",
        "    images = images_full_res\n",
        "\n",
        "  # Remove the full-resolution versions from memory (just clogging things up)\n",
        "  del images_full_res\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "#resize(train_images, shape=(331,331))\n",
        "#resize(test_images, shape=(331,331))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkcCHU6dOqtw",
        "colab_type": "text"
      },
      "source": [
        "## Data Visualization\n",
        "\n",
        "Run the cell below this one to see what the images you are working with look like. They are 299x299 pixel greyscale images of sections of mammograms. Generally, masses and calcifications show up as blobs of various size, shape, and brightness. As mentioned already, dense tissue shows up brighter on mammograms and it can be dificult to spot an abnormality in mammograms with denser tissue. Each image is labeled with its binary and multiclass labels. Notice that some images are not tissue at all but instead are segments of the mammogram labels (image 10 for example). Further data preprocessing or a robust classifier should successfully deal with these images. Rerun this cell to look at other images in the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exvqx8rZhFuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "73e39f4c-2034-4aa0-d56f-b0f60954837f"
      },
      "source": [
        "### Visualize Images in Training Set\n",
        "'''\n",
        "fig = plt.figure(figsize = (16,16))\n",
        "\n",
        "for i in range(9): # change these numbers to explore the training dataset further\n",
        "  imgInd = np.random.randint(0, len(train_images_df))\n",
        "  ax = plt.gcf().add_subplot(3, 3, i+1)\n",
        "  ax.imshow(train_images[imgInd], cmap='gray', vmin=0, vmax=255)\n",
        "  ax.set_title(\"data index: \" + str(imgInd) + \"\\nlabel_binary: \" +\n",
        "            str(train_binary_labels_df.iloc[imgInd,0]) +\n",
        "            '\\nlabel_multiclass: ' + str(train_multiclass_labels_df.iloc[imgInd,0]))\n",
        "  \n",
        "  #set major tick marks every 50 pixels\n",
        "  ax.xaxis.set_major_locator(MultipleLocator(50))\n",
        "  ax.yaxis.set_major_locator(MultipleLocator(50))\n",
        "\n",
        "  #turn on grid lines\n",
        "  ax.grid(True)\n",
        "\n",
        "fig.tight_layout()\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfig = plt.figure(figsize = (16,16))\\n\\nfor i in range(9): # change these numbers to explore the training dataset further\\n  imgInd = np.random.randint(0, len(train_images_df))\\n  ax = plt.gcf().add_subplot(3, 3, i+1)\\n  ax.imshow(train_images[imgInd], cmap=\\'gray\\', vmin=0, vmax=255)\\n  ax.set_title(\"data index: \" + str(imgInd) + \"\\nlabel_binary: \" +\\n            str(train_binary_labels_df.iloc[imgInd,0]) +\\n            \\'\\nlabel_multiclass: \\' + str(train_multiclass_labels_df.iloc[imgInd,0]))\\n  \\n  #set major tick marks every 50 pixels\\n  ax.xaxis.set_major_locator(MultipleLocator(50))\\n  ax.yaxis.set_major_locator(MultipleLocator(50))\\n\\n  #turn on grid lines\\n  ax.grid(True)\\n\\nfig.tight_layout()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8MjAxxREoFb",
        "colab_type": "text"
      },
      "source": [
        "# Example MULTICLASS CNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LeuKhijbBVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Create one-hot labels\n",
        "train_multiclass_labels = train_multiclass_labels_df[\"Label\"]\n",
        "\n",
        "#convert labels to onehot, ensure type is float32\n",
        "train_multiclass_labels = tf.keras.utils.to_categorical(train_multiclass_labels, 5).astype(np.float32)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu2FiweZGHL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Shuffle and partition labeled data\n",
        "\n",
        "train_images_shuffled, train_multiclass_labels_shuffled = shuffle(train_images, train_multiclass_labels, random_state = 25, stratify = train_multiclass_labels_df[\"Label\"])\n",
        "train_images_shuffled = train_images_shuffled.astype(np.float16)/255.0\n",
        "\n",
        "train_images_shuffled = tf.constant(train_images_shuffled, dtype=tf.float16)\n",
        "\n",
        "# Take note of number of color channels in the loaded image add a last axis to \n",
        "# images ndarray if array dimension is only 3 (as is the case with grayscale images)\n",
        "'''\n",
        "if train_images_shuffled.ndim == 3:\n",
        "    # If image is grayscale, then we add a last axis (of len 1) for channel\n",
        "    n_channels = 1\n",
        "    train_images_shuffled = train_images_shuffled[:, : , :, np.newaxis]\n",
        "else:\n",
        "    #if image is not grayscale, last dimension of image already corresponds to channel\n",
        "    n_channels = train_images_shuffled.shape[-1]'''\n",
        "\n",
        "#train_images_shuffled = np.repeat(train_images_shuffled[...,np.newaxis],3,-1)\n",
        "'''\n",
        "train_images_shuffled = tf.constant(train_images_shuffled, dtype=tf.float16)\n",
        "train_images_shuffled = tf.image.grayscale_to_rgb(train_images_shuffled)\n",
        "'''\n",
        "\n",
        "#--Partition into train/val/mock_test--\n",
        "\n",
        "#Note that in a 2D convolution, we need another dimension on all \n",
        "#train/val/mocktest data. This dimension corresponds to \"number of channels\"\n",
        "#(an RGB image would have 3 channels, for example). Our images only have one \n",
        "#channel, but the 2D convolution layers still expect the last data dimension to \n",
        "#correspond to channel. (Without this added channel, error messages would result\n",
        "#saying the expected number of dimensions into the 2D convolution layer is 4,\n",
        "#and not 3. Feel free to try for yourself.)\n",
        "\n",
        "\n",
        "val_size = 1000\n",
        "mocktest_size = 500\n",
        "\n",
        "mocktest_data = train_images_shuffled[0:mocktest_size, :, :]\n",
        "mocktest_multiclass_labels = train_multiclass_labels_shuffled[0:mocktest_size, :]\n",
        "\n",
        "val_data = train_images_shuffled[mocktest_size:mocktest_size+val_size, :, :]\n",
        "val_multiclass_labels = train_multiclass_labels_shuffled[mocktest_size:mocktest_size+val_size, :]\n",
        "\n",
        "partial_train_data = train_images_shuffled[mocktest_size+val_size:,:,:]\n",
        "tr_multiclass_labels = train_multiclass_labels_shuffled[mocktest_size+val_size:,:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxuOj20iVez3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unfreeze_layers(model, top_n_layers):\n",
        "  for layer in model.layers[-top_n_layers:]:\n",
        "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "      layer.trainable = True\n",
        "\n",
        "def freeze_layers(model, top_n_layers):\n",
        "  for layer in model.layers[-top_n_layers:]:\n",
        "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "      layer.trainable = False"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppBjv6ST83FC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dc5fc927-1d86-4857-b404-f991fa4fc909"
      },
      "source": [
        "### Define a convolutional neural network structure and compile mode\n",
        "'''\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape = (partial_train_data.shape[1:])))\n",
        "model.add(tf.keras.layers.MaxPooling2D(strides=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(32,(3,3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax)) \n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics='accuracy', loss_weights = [1,1,1,3,3], run_eagerly=True)\n",
        "\n",
        "model.summary()'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ntf.keras.backend.clear_session()\\n\\nmodel = tf.keras.Sequential()\\nmodel.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape = (partial_train_data.shape[1:])))\\nmodel.add(tf.keras.layers.MaxPooling2D(strides=(2,2)))\\nmodel.add(tf.keras.layers.Conv2D(32,(3,3), activation='relu'))\\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\\nmodel.add(tf.keras.layers.Flatten())\\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\\nmodel.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax)) \\n\\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\\n\\n#model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics='accuracy', loss_weights = [1,1,1,3,3], run_eagerly=True)\\n\\nmodel.summary()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LilUZL0Y4qFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "23ecea69-1817-42a3-d891-0eb955ffff81"
      },
      "source": [
        "'''loss function to limit false negatives from stack overflow'''\n",
        "'''\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "\n",
        "def binary_recall_specificity(y_true, y_pred, recall_weight, spec_weight):\n",
        "\n",
        "    TN = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 0)\n",
        "    TP = np.logical_and(K.eval(y_true) == 1, K.eval(y_pred) == 1)\n",
        "\n",
        "    FP = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 1)\n",
        "    FN = np.logical_and(K.eval(y_true) == 1, K.eval(y_pred) == 0)\n",
        "\n",
        "    # Converted as Keras Tensors\n",
        "    #TN = K.sum(K.variable(TN))\n",
        "    #FP = K.sum(K.variable(FP))\n",
        "\n",
        "    specificity = TN / (TN + FP + K.epsilon())\n",
        "    recall = TP / (TP + FN + K.epsilon())\n",
        "\n",
        "    return 1.0 - (recall_weight*recall + spec_weight*specificity)\n",
        "  \n",
        "# Our custom loss' wrapper\n",
        "def custom_loss(recall_weight, spec_weight):\n",
        "\n",
        "    def recall_spec_loss(y_true, y_pred):\n",
        "        return binary_recall_specificity(y_true, y_pred, recall_weight, spec_weight)\n",
        "\n",
        "    # Returns the (y_true, y_pred) loss function\n",
        "    return recall_spec_loss\n",
        "\n",
        "recall_loss=custom_loss(recall_weight=0.95, spec_weight=0.05)'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport numpy as np\\nimport keras.backend as K\\n\\ndef binary_recall_specificity(y_true, y_pred, recall_weight, spec_weight):\\n\\n    TN = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 0)\\n    TP = np.logical_and(K.eval(y_true) == 1, K.eval(y_pred) == 1)\\n\\n    FP = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 1)\\n    FN = np.logical_and(K.eval(y_true) == 1, K.eval(y_pred) == 0)\\n\\n    # Converted as Keras Tensors\\n    #TN = K.sum(K.variable(TN))\\n    #FP = K.sum(K.variable(FP))\\n\\n    specificity = TN / (TN + FP + K.epsilon())\\n    recall = TP / (TP + FN + K.epsilon())\\n\\n    return 1.0 - (recall_weight*recall + spec_weight*specificity)\\n  \\n# Our custom loss' wrapper\\ndef custom_loss(recall_weight, spec_weight):\\n\\n    def recall_spec_loss(y_true, y_pred):\\n        return binary_recall_specificity(y_true, y_pred, recall_weight, spec_weight)\\n\\n    # Returns the (y_true, y_pred) loss function\\n    return recall_spec_loss\\n\\nrecall_loss=custom_loss(recall_weight=0.95, spec_weight=0.05)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jAV3Ci5VaE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "9d41820e-297f-4765-bf49-67f39b87c53a"
      },
      "source": [
        "'''Model 2: INCEPTION'''\n",
        "tf.keras.backend.clear_session()\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                input_shape=(partial_train_data.shape[1:]),\n",
        "                                                weights='imagenet')\n",
        "\n",
        "base_model.trainable=False\n",
        "#unfreeze_layers(base_model, 5)\n",
        "\n",
        "# Add a global spatial average pooling layer\n",
        "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "# Add a fully-connected layer\n",
        "dense_layer = tf.keras.layers.Dense(32, activation = 'relu')\n",
        "\n",
        "# Add the final classification layer\n",
        "final = tf.keras.layers.Dense(5, activation='softmax')\n",
        "\n",
        "# Build the model you will train\n",
        "model = tf.keras.Sequential([base_model, global_avg_layer, dense_layer, final])\n",
        "\n",
        "#modified_cce = tf.keras.losses.CategoricalCrossentropy\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics='accuracy', loss_weights = [14,8,8,9,9], run_eagerly=True)\n",
        "\n",
        "# Print summary of model layers\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                65568     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 21,868,517\n",
            "Trainable params: 65,733\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUsjUdP8SfOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "40d32b2f-fa14-4d72-b8e6-4ae440ba8865"
      },
      "source": [
        "### Train Model\n",
        "#This function is called after each epoch\n",
        "#(It will ensure that your training process does not consume all available RAM)\n",
        "class garbage_collect_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.2, min_delta=0.00000001)\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(partial_train_data, # Train examples\n",
        "          tr_multiclass_labels, # Train labels\n",
        "          epochs=50, # number of epochs (passes through data during training)\n",
        "          batch_size= 10, # number of points to consider in each optimizer iteration\n",
        "          callbacks = [garbage_collect_callback(), early_stop, reduce_lr],\n",
        "          validation_data=(val_data, val_multiclass_labels), #data to use for validation\n",
        "          verbose=1) #will print information about optimization process\n",
        "\n",
        "#-- Evaluate model using mocktest_data --\n",
        "#Warning: the value returned by model.evaluate depends on what metrics were given\n",
        "#to the model at compile time. Thus, if the metric is no longer accuracy, the\n",
        "#label \"model accuracy\" below will no longer be accurate\n",
        "\n",
        "\n",
        "test_binary_pred = model.predict(mocktest_data)\n",
        "scores = model.evaluate(mocktest_data, mocktest_multiclass_labels, verbose = 0)\n",
        "print('\\nTesting model on mock_test set:')\n",
        "print(f'Model Loss: {scores[0]:.3f}, Model Accuracy: {scores[1]:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "102/400 [======>.......................] - ETA: 1:12 - loss: 18.4653 - accuracy: 0.5049"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ultum1UWTdG7",
        "colab_type": "text"
      },
      "source": [
        "# Submitting your model\n",
        "\n",
        "Once you have finished creating your classifier, you need to use it to classify the test images and make a pickle file containing your models predictions stored as a pandas dataframe. Instructions for how to build the pandas dataframe containing your model's predictions are included below. (Note that the process is demonstrated for the example classifier in the next code cell to help clarify these instructions.)\n",
        "\n",
        "* Create pandas dataframe with 4 columns (or 7 columns for complex lables). One column should contain your model's prediction for each image (represented as an integer) and one column should contain the unique index for that image in the test data (recall that test data indices range from 5500 to 5999). The other columns in the dataframe should contain the probability that the image is classified in each class. **Make sure the unique indices on your dataframe match the unique indices of the images being predicted. The order of rows and columns do not matter, as long as there are 1500 rows and either 4 or 7 columns (depending on which classifier you have made)**. (The below cell demonstrates how to build this dataframe created using the example classifier. You can refer to this code for more clarification of these instructions.)\n",
        " * If your classifier does not compute the probabilities for each class and simply outputs an integer label for the prediction (e.g. 0, 1, 2, 3, or 4 for multiclass classification), the probability for the predicted class should be 1.00 and the probability for all other class(es) should be 0.00 in the dataframe.\n",
        "* Download the pandas dataframe as a pickle file using pickle.dump() and files.download(). (See below cell for clarification on this procedure.)\n",
        "* Submit your pickle file to the submissions channel on the Medlytics 2020 discord channel. You can submit up to 3 times before your final evaluation.\n",
        "\n",
        "Your model will be evaluated on confusion matrix score (explained below), area under the ROC curve (ROCAUC), and creativity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8phZzYxs5rm",
        "colab_type": "text"
      },
      "source": [
        "# Example code for saving model predictions on the test set (for submission)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntbJLmh-CrBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Use the created neural network model to create a set of predictions for the test set\n",
        "\n",
        "#Refer to the dataframe produced by this cell to clarify correct formatting when\n",
        "#submitting pickle files for week 3 challenge (This cell produces dataframe\n",
        "#using the example CNN classifier, but the same dataframe should be produced for\n",
        "#any type of classifier.)\n",
        "\n",
        "#use model.predict to create a set of predictions (onehot encoded)\n",
        "test_predictions_onehot = model.predict(test_images[:,:,:])\n",
        "\n",
        "def create_test_predictions_df_for_submission(test_predictions_onehot):\n",
        "    test_predictions_df = pd.DataFrame(test_predictions_onehot)\n",
        "\n",
        "    #add a column to dataframe containing the unique index for each sample\n",
        "    test_predictions_df = pd.concat((test_predictions_df, test_images_df[\"Unique_Index\"]), axis = 1)\n",
        "\n",
        "    #add another column to dataframe with an integer representing the guessed class for that example\n",
        "    #(Note this is the number corresponding to the class with highest probability score)\n",
        "    guesses = [np.argmax(test_predictions_onehot[i,:]) for i in range(test_predictions_onehot.shape[0])]\n",
        "    test_predictions_df = pd.concat((test_predictions_df, pd.DataFrame(guesses, columns = ['predicted_class'])), axis = 1)\n",
        "\n",
        "    return test_predictions_df\n",
        "\n",
        "test_predictions_df = create_test_predictions_df_for_submission(test_predictions_onehot)\n",
        "print('This is what the appropriately formatted dataframe should look like:')\n",
        "print('(4 columns total for binary classification, and 7 for multiclass classification)\\n')\n",
        "print(test_predictions_df.head())\n",
        "print('\\n\\n')\n",
        "\n",
        "fname = 'Multiclass Attempt 1.p'\n",
        "pickle.dump(test_predictions_df, open(fname, 'wb')) # Creates pickle file in Google Colab but doesn't save to your machine\n",
        "    \n",
        "# Saving to pickle file to local drive (should be in Downloads). \n",
        "# Submit via the Medlytics 2020 \"submissions\" channel on Discord\n",
        "files.download(fname) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S-gd_slKJLc",
        "colab_type": "text"
      },
      "source": [
        "# How Your Model Will Be Evaluated\n",
        "\n",
        " \n",
        "* **Confusion Matrix Score**: You will receive a certain number of points for each correct classification and a certain point penalty for each incorrect classification. The point reward scheme is shown below for both binary and multiclass classifiers. This reward scheme punishes less crucial misclassifications (e.g., misclassifying a Negative scan as a Benign Calcification) much less than it punishes costly misclassifications (e.g., misclassifying a Malignant Calcification as a Negative). In this way, our evaluation metric reflects the practical needs of the classifier in its application to real-world problems. It is our hope that you keep these differential punishments in mind when creating your classifiers. \n",
        "\n",
        "<h3><center>Multi-class Confusion Matrix Scoring</center></h3>\n",
        "<center>\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week3/Week3_Challenge_ConfMatrixTable.svg\" width = 500>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<h3><center>Binary Confusion Matrix Scoring</center></h3>\n",
        "<center>\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week3/Week3_Challenge_ConfMatrixTable_binary.svg\" width = 250>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "* **Area Under ROC Curve:** The receiver operating characteristic (ROC) curve plots the true positive rate (sensitivity/recall) against the false positive rate (fall-out) at many decision threshold settings. The area under the curve (AUC) measures discrimination, the classifier's ability to correctly identify samples from the \"positive\" and \"negative\" cases. Intuitively, AUC is the probability that a randomly chosen \"positive\" sample will be labeled as \"more positive\" than a randomly chosen \"negative\" sample. In the case of a multi-class ROC curve, each class is considered separately before taking the weighted average of all the class results. Simply put, the class under consideration is labeled as \"positive\" while all other classes are labeled as \"negative.\" The AUCROC score should be between 0.5 and 1, in which 0.5 is random classification and 1 is perfect classification.\n",
        "\n",
        "<h3><center>Example ROC Curve</center></h3>\n",
        " <center><img src=\"http://fastml.com/images/auc/roc_curve.png\"></center>\n",
        "\n",
        "(In the image above, the dashed line represents a classifier that classifies based purely on chance. The perfect classifier will have a line that passes through the top left corner of the graph (true positive rate = 1, false positive rate = 0) and will have a ROCAUC (ROC Area Under Curve) of 1.)\n",
        " \n",
        "\n",
        " \n",
        " Using the test data, the example classifier above performs thus on these metrics:\n",
        "  * Confusion Matrix Score: 757\n",
        "  * ROCAUC: 0.827\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b7MS4h1DNiT",
        "colab_type": "text"
      },
      "source": [
        "## Example code showing calculation of evaluation metrics (binary classifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m0kCY5mZsse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use ground_truth_labels and predicted_labels to calculate confusion matrix\n",
        "#\n",
        "# Note: When your model is tested, the ground_truth_labels will be the test_labels\n",
        "# and the predicted_labels will be your models predicted labels on the test set.\n",
        "# To demonstrate the calculation however, we will use other data to populate \n",
        "# ground_truth_labels and predicted_labels.\n",
        "#\n",
        "# Variable definitions\n",
        "#-------------------------------------\n",
        "# ground_truth_labels - 1D array where ith element is an integer representing \n",
        "# the true label of example i\n",
        "#\n",
        "# predicted_labels - 1D array where ith element is an integer representing the\n",
        "# predicted class of example i\n",
        "\n",
        "#In this cell we treat mocktest_data and mocketst_binary_labels as if it were\n",
        "#the real data used to score the preliminary binary classification model\n",
        "\n",
        "\n",
        "#Note: the argmax procedure effectively converts back from onehot encoding to \n",
        "#numerical encoding. We need to do this for the true labels and for the \n",
        "#predicted labels to use confusion_matrix function\n",
        "ground_truth_onehot = mocktest_multiclass_labels\n",
        "ground_truth_labels = np.array([np.argmax(ground_truth_onehot[i, :])\n",
        "                               for i in range(mocktest_multiclass_labels.shape[0])])\n",
        "\n",
        "predicted_onehot = model.predict(mocktest_data)\n",
        "predicted_labels = np.array([np.argmax(predicted_onehot[i, :]) \n",
        "                            for i in range(predicted_onehot.shape[0])], dtype = np.int8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQq_lY-ccStc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##--Confusion Matrix Score for Binary Classifier--\n",
        "\n",
        "binary_confmat_weights = np.array([[2, -3],\n",
        "                                  [-6, 2]])\n",
        "\n",
        "confmat = confusion_matrix(ground_truth_labels, predicted_labels) # confusion_matrix is a function from sklearn.metrics and it automatically\n",
        "                                                                  # constructs a confusion matrix for any number of classes \n",
        "                                                                  # Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n",
        "print(confmat)\n",
        "\n",
        "# Simply multiply two matrices elementwise, and then sum all the results\n",
        "confmat_score = np.sum(confmat * binary_confmat_weights)\n",
        "\n",
        "#--ROCAUC for Binary Classifier--\n",
        "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(ground_truth_labels, predicted_labels, pos_label=1)\n",
        "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)    # roc_curve() and auc() are also functions from sklearn.metrics. roc_curve calculates true and flase positive\n",
        "                                                                  # rates over many theshold values. pos_label = 1 specifies that in the labels a 1\n",
        "                                                                  # denotes a positive example (and anything less denotes a negative example). auc() simply calculates the AUC.\n",
        "                                                                  # Documentation:\n",
        "                                                                  # roc_curve(): http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
        "                                                                  # auc(): http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
        "\n",
        "print(f'Confusion Matrix Score: {confmat_score}, ROCAUC: {roc_auc:.3f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WmcuzMdgVSN",
        "colab_type": "text"
      },
      "source": [
        "## Example code showing calculation of evaluation metrics (multiclass classifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWzmgwePgW0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use ground_truth_labels and predicted_labels to calculate confusion matrix\n",
        "#\n",
        "# Note: When your model is tested, the ground_truth_labels will be the test_labels\n",
        "# and the predicted_labels will be your models predicted labels on the test set.\n",
        "# To demonstrate the calculation however, we will use other data to populate \n",
        "# ground_truth_labels and predicted_labels.\n",
        "#\n",
        "# Variable definitions\n",
        "#-------------------------------------\n",
        "# ground_truth_labels - 1D array where ith element is an integer representing \n",
        "# the true label of example i\n",
        "#\n",
        "# predicted_labels - 1D array where ith element is an integer representing the\n",
        "# predicted class of example i\n",
        "\n",
        "# Because a classifier wasn't trained for the multiclass problem (in the starter\n",
        "# code anyway) we will quickly define ground_truth_labels and predicted_labels\n",
        "# with dummy values\n",
        "\n",
        "#ground_truth_labels = np.array([i for i in range(1500)])//300 #Labels range from 0 to 4\n",
        "#ground_truth_labels_onehot = np.eye(5)[ground_truth_labels.astype(np.uint8)]\n",
        "\n",
        "#predicted_labels = predicted_labels #prediction is always class 0 (this is a very naiive predictor)\n",
        "#prediction_scores = np.eye(5)[predicted_labels] #Row i of this matrix is the likelyhood 'score' for example i in each of the classes\n",
        "ground_truth_onehot = mocktest_multiclass_labels\n",
        "ground_truth_labels_onehot = np.array([np.argmax(ground_truth_onehot[i, :])\n",
        "                               for i in range(mocktest_multiclass_labels.shape[0])])\n",
        "\n",
        "predicted_onehot = model.predict(mocktest_data)\n",
        "predicted_labels = np.array([np.argmax(predicted_onehot[i, :]) \n",
        "                            for i in range(predicted_onehot.shape[0])], dtype = np.int8)\n",
        "prediction_scores = np.eye(5)[predicted_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3HyjrjKoYBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--Confusion Matrix Score for Multiclass Classifier--\n",
        "\n",
        "multiclass_confmat_weights = np.array([[2,-1,-1,-3,-3],\n",
        "                                    [-2,2,-1,-3,-3],\n",
        "                                    [-2,-1,2,-3,-3],\n",
        "                                    [-6,-4,-4,2,-2],\n",
        "                                    [-6,-4,-4,-2,2]])\n",
        "\n",
        "confmat = confusion_matrix(ground_truth_labels_onehot, predicted_labels)\n",
        "  \n",
        "confmat_score = np.sum(confmat * multiclass_confmat_weights)\n",
        "\n",
        "\n",
        "      \n",
        "#--ROCAUC for Multiclass Classifier--\n",
        "  # ROCAUC for multiclass classifiers are calculated by treating each class as a binary classification (this class or all other classes) and micro-averaging \n",
        "  # each class's ROCAUC to get a total ROCAUC\n",
        "false_positive_rate = dict()\n",
        "true_positive_rate = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(5):  # looping through each of 5 classes and getting false and true positive rates and calculating ROCAUC for each class\n",
        "  false_positive_rate[i], true_positive_rate[i], _ = metrics.roc_curve(ground_truth_onehot[:,i], prediction_scores[:,i]) # Because labels are not binary, \n",
        "  roc_auc[i] = metrics.auc(false_positive_rate[i], true_positive_rate[i])                                                    # they need to be One Hot Vectors (ex. 2 = [0,0,1,0,0])\n",
        "                                                                                                                     # prediction_scores are the predicted 'probability'\n",
        "                                                                                                                     # that the example belongs to each of the 5 classes\n",
        "false_positive_rate[\"micro\"], true_positive_rate[\"micro\"], _ = metrics.roc_curve(ground_truth_onehot.ravel(), prediction_scores.ravel())\n",
        "roc_auc = metrics.auc(false_positive_rate[\"micro\"], true_positive_rate[\"micro\"]) # micro-averaging averages the ROCAUCs from each class weighted \n",
        "                                                                         # according to number of example in that class\n",
        "   \n",
        "print(f'Multiclass Confusion Matrix Score: {confmat_score}, c: {roc_auc:.3f}')\n",
        "print(confmat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AnLjOyK201K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm_norm = confmat.astype('float')/confmat.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "def plot_cmatrix(cm, labels, title='Confusion Matrix'):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  cax = ax.matshow(cm, cmap='Reds')\n",
        "  plt.title('\\n'+title+'\\n', fontsize=20)\n",
        "  fig.colorbar(cax)\n",
        "  ax.set_yticklabels(['']+labels, fontsize=16)\n",
        "  ax.set_xticklabels(['']+labels, fontsize=16)\n",
        "  plt.xlabel('Predicted', fontsize=16)\n",
        "  plt.ylabel('True', fontsize=16)\n",
        "  plt.show()\n",
        "\n",
        "plot_cmatrix(cm_norm, [0,1,2,3,4], title='Normalized Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkJcTaUvc2uG",
        "colab_type": "text"
      },
      "source": [
        "# Troubleshooting tips\n",
        "\n",
        "- If you try to plot the images using matplotlib  you may notice that it will invert the grayscale values when it plots. You can fix this by plotting 255 - pixel_values or by adding the argument `cmap='gray'` to your imshow function (as I did when visualizing the images).\n",
        "\n",
        "- If you get an \"ResourcesExhausted\" error, this means you have used all the GPU resources dedicated to your session and you will need to restart your runtime (Runtime > Restart Runtime). After restarting your runtime your environment will be wiped so you will need to reload the data.\n",
        "\n",
        "- If you get a warning window that says you are close to reaching the session's memory limit, this means you are nearing a ResourcesExhausted error. You can still run code until the error occurs, but note that you will likely need to restart your runtime soon.\n",
        "\n",
        "- ResourcesExhausted errors will only occur if you're using GPU support.\n",
        "\n",
        "- Google Colab will end your runtime automatically after 24 hours\n",
        "\n",
        "- If things seem really slow, double check that you're using the GPU\n",
        "\n",
        "- If you are close to using all available ram, try to double check the memory being consumed by different variables using the provided cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyavuR7Gz8nP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "7b520d5b-c9bc-4ba9-c313-118791a9dbbd"
      },
      "source": [
        "# If you find yourself running out of ram below is a useful method to print out \n",
        "# the sizes of the variables in your workspace\n",
        "\n",
        "import sys\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "                         key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
        "\n",
        "type(train_images[0,:,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  train_images:  1.4 GiB\n",
            "                   test_images: 383.7 MiB\n",
            "               train_images_df: 773.6 KiB\n",
            "                test_images_df: 211.1 KiB\n",
            "       train_multiclass_labels: 107.5 KiB\n",
            "train_multiclass_labels_shuffled: 107.5 KiB\n",
            "        train_binary_labels_df: 86.1 KiB\n",
            "    train_multiclass_labels_df: 86.1 KiB\n",
            "    ground_truth_labels_onehot: 58.7 KiB\n",
            "             prediction_scores: 58.7 KiB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}