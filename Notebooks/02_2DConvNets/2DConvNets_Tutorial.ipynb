{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2DConvNets_Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a7scg4EGMfYk"
      },
      "source": [
        "# Building a 2D CNN for Image Classification\n",
        "In this tutorial, we will be learning how to build a 2D version of a Convolutional Nerual Network (CNN).\n",
        "\n",
        "\n",
        "For this example, we will again be using the same dataset of histological images: http://dx.doi.org/10.5281/zenodo.53169. \n",
        "\n",
        "![Example histological images for classes a through f](https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/Images/Week3/Representative-images-the-first-10-images-of-every-tissue-class.png?token=ADGRCSE2YIYWUNCJTHNB5XK67T4HE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GESM76HY9Dea"
      },
      "source": [
        "# Imports\n",
        "Start by importing the packages and modules we will be needing for this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W86Oi6dCMfYp",
        "colab": {}
      },
      "source": [
        "# Basic operating system (os), numerical, and plotting functionality\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# scikit-learn data utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "import skimage.transform as image_transform\n",
        "\n",
        "# scikit-learn performance metric utilities\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Import our neural network building tools\n",
        "import tensorflow as tf\n",
        "\n",
        "# Garbage collection (for saving RAM during training)\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ta7q5xwIllSi"
      },
      "source": [
        "# Retrieve and Load the Data\n",
        "The following cell checks if the histological images have already been downloaded into the current Colab session and downloads them if not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KJvH96N6J4tP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2662e149-abc1-4f9b-8759-3b296dc279a2"
      },
      "source": [
        "# Define the current directory and the directory where the files to download can\n",
        "# be found\n",
        "current_dir = '/content'\n",
        "remote_path = 'https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/'\n",
        "\n",
        "# Define and build a directory to save this data in\n",
        "data_dir = '/content/crc_data'\n",
        "if not os.path.isdir(data_dir):\n",
        "  os.mkdir(data_dir)\n",
        "\n",
        "# Move into the data directory and download all of the files\n",
        "os.chdir(data_dir)\n",
        "for ii in range(1, 6):\n",
        "    basename = f'rgb0{ii}.npz'\n",
        "    filename = os.path.join(remote_path, basename)\n",
        "\n",
        "    # Check if the file has already been downloaded\n",
        "    if not os.path.isfile(basename):\n",
        "      cmd = f'wget {filename}'\n",
        "      print(cmd)\n",
        "      os.system(cmd)\n",
        "\n",
        "# Return to the original directory\n",
        "os.chdir(current_dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb01.npz\n",
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb02.npz\n",
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb03.npz\n",
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb04.npz\n",
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb05.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tDGLNYFqJ90l"
      },
      "source": [
        "The downloaded \".npz\" archives can now be read into memory and parsed using our data loading function copied from the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y9rC7eJMllSv",
        "colab": {}
      },
      "source": [
        "# Define a function to load the data from the assumed download path\n",
        "def load_images(colorspace='rgb'):\n",
        "    \"\"\"\n",
        "    Loads the example data and applies transformation into requested colorspace\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    colorspace : str, optional, default: `rgb`\n",
        "        The colorspace into which the images should be transformed. Accepted\n",
        "        values include\n",
        "\n",
        "        'rgb' : Standard red-green-blue color-space for digital images\n",
        "\n",
        "        'gray' or 'grey': An arithmetic average of the (r, g, b) values\n",
        "\n",
        "        'lab': The CIE L* a* b* colorspace\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    images : numpy.ndarray, shape (Nimg, Ny, Nx, Ncolor)\n",
        "        The complete set of transformed images\n",
        "\n",
        "    labels : numpy.ndarray, shape (Nimg)\n",
        "        The classification labels associated with each entry in `images`\n",
        "\n",
        "    label_to_str : dict\n",
        "        A dictionary which converts the numerical classification value in\n",
        "        `labels` into its string equivalent representation.\n",
        "    \"\"\"\n",
        "    # Check that the colorspace argument is recognized\n",
        "    colorspace_lower = colorspace.lower()\n",
        "    if colorspace_lower not in ['rgb', 'gray', 'grey', 'lab']:\n",
        "        raise ValueError(f'`colorspace` value of {colorspace} not recognized')\n",
        "\n",
        "    # Load data, which is stored as a numpy archive file (.npz)\n",
        "    filename = os.path.join(data_dir, 'rgb01.npz')\n",
        "    print(f'loading {filename}')\n",
        "    tmp = np.load(os.path.join(data_dir, 'rgb01.npz'), allow_pickle=True)\n",
        "\n",
        "    # Parse the loaded data into images and labels\n",
        "    # Initialize the images and labels variables using the first archive data\n",
        "    images = tmp['rgb_data']\n",
        "    if colorspace_lower == 'rgb':\n",
        "        pass\n",
        "    elif colorspace_lower in ['gray', 'grey']:\n",
        "        images = np.mean(images, axis=-1)      # Average into grayscale\n",
        "    elif colorspace_lower == 'lab':\n",
        "        images = rgb2lab(images)               # Convert to CIE L*a*b*\n",
        "\n",
        "    # Grab the initial array for the image labels\n",
        "    labels = tmp['labels']\n",
        "    \n",
        "    # Grab the dictionary to convert numerical labels to their string equivalent\n",
        "    label_to_str = tmp['label_str']\n",
        "    label_to_str = label_to_str.tolist() # Convert label_to_str into a dict\n",
        "\n",
        "    # Update the user on the number and size of images loaded\n",
        "    print('Loaded images with shape {}'.format(images.shape))\n",
        "    del tmp\n",
        "\n",
        "    # Loop over each of the remaining archives and append the contained data\n",
        "    for ii in range(2,6):\n",
        "        # Build the full path to the archive and load it into memory\n",
        "        filename = os.path.join(data_dir, f'rgb0{ii}.npz')\n",
        "        print(f'loading {filename}')\n",
        "        tmp = np.load(filename, allow_pickle=True)\n",
        "\n",
        "        # Parse and append the data\n",
        "        these_images = tmp['rgb_data']\n",
        "        if colorspace_lower == 'rgb':\n",
        "            pass\n",
        "        elif (colorspace_lower == 'gray') or (colorspace_lower == 'grey'):\n",
        "            these_images = np.mean(these_images, axis=-1) # Convert to grayscale\n",
        "        elif colorspace_lower == 'lab':\n",
        "            these_images = rgb2lab(these_images)          # Convert to CIEL*a*b*\n",
        "\n",
        "        # Append the images and labels\n",
        "        images = np.append(images, these_images, axis=0)\n",
        "        labels = np.append(labels, tmp['labels'], axis=0)\n",
        "\n",
        "        # Update the user on the number and size of images\n",
        "        print('Loaded images with shape {}'.format(these_images.shape))\n",
        "        del tmp\n",
        "\n",
        "    # Force the image data to be floating point and print the data shape\n",
        "    images = images.astype(np.float)\n",
        "    print('Final image data shape: {}'.format(images.shape))\n",
        "    print('Number of image labels: {}'.format(*labels.shape))\n",
        "\n",
        "    return images, labels, label_to_str"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1LGPaPqqrSoQ"
      },
      "source": [
        "With this data-loading function defined, let's load in the grayscale images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsJAjmJgM1Od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "dd604f8b-59af-4c02-abee-8d62d290834e"
      },
      "source": [
        "# Load the data using our data-loading function\n",
        "#\n",
        "# You can change the `colorspace` keyword argument in `load_images` function\n",
        "# call to try to try training classifier an image in another colorspace\n",
        "images_full_res, labels, label_to_str = load_images(colorspace='gray')\n",
        "num_classes = np.unique(labels).size"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading /content/crc_data/rgb01.npz\n",
            "Loaded images with shape (1000, 150, 150)\n",
            "loading /content/crc_data/rgb02.npz\n",
            "Loaded images with shape (1000, 150, 150)\n",
            "loading /content/crc_data/rgb03.npz\n",
            "Loaded images with shape (1000, 150, 150)\n",
            "loading /content/crc_data/rgb04.npz\n",
            "Loaded images with shape (1000, 150, 150)\n",
            "loading /content/crc_data/rgb05.npz\n",
            "Loaded images with shape (1000, 150, 150)\n",
            "Final image data shape: (5000, 150, 150)\n",
            "Number of image labels: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1_mJOsLM5J-",
        "colab_type": "text"
      },
      "source": [
        "## Pre-process the Images\n",
        "\n",
        "The following cells apply some pre-processing steps to the image data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy0cWNN3aOGa",
        "colab_type": "text"
      },
      "source": [
        "### Resize the Images for Faster Training\n",
        "\n",
        "In the following cell, we resize the images so that they are lower resolution than the originals. This only has a minimal impact on the classifier performance, but it significantly accelerates training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3ue6DZ6M23D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab the original shape of the images\n",
        "num_images = images_full_res.shape[0]\n",
        "\n",
        "# Specify a desired resizing shape.\n",
        "# NOTE: This can be modified to trade-off between training-speed and performance\n",
        "resize_shape = (48, 48)\n",
        "\n",
        "# Initialize an array for storing the resized images\n",
        "images = np.zeros((num_images,) + resize_shape)\n",
        "\n",
        "# Loop over each image in the data and perform a resizing operation\n",
        "for img_num, img_data in enumerate(images_full_res):\n",
        "    images[img_num] = image_transform.resize(img_data, resize_shape)\n",
        "\n",
        "# Remove the full-resolution versions from memory (just glogging things up)\n",
        "del images_full_res"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PtlCeN4FMfZD"
      },
      "source": [
        "## Normalize the Image Data\n",
        "All images should be normalized to the range $\\left[0, 1\\right]$. The procedure for this will depend on the current colorspace of the images, but some basic guidelines follow:\n",
        "\n",
        "* **RGB and greyscale:** images are often represented with integers in the range $\\left[0, 255\\right]$, so dividing by $255$ will often normalize these images (if they are not already normalized).\n",
        "\n",
        "* **HSV, HSI, HSL:** the Hue (H) channel will typically be represnted by a number in the range $\\left[0^{\\circ}, 360^{\\circ}\\right)$. The Saturation channel channels will often be represented on a scale from $\\left[0, 1\\right]$, in which case no normalization is required. The final channel (Value, Intensity, or Lightness) is often represented by a snumber in the range of either $\\left[0, 1\\right]$ or $\\left[0, 100\\right]$, so a simple investigation should indicate what normalization may be needed for that channel.\n",
        "\n",
        "* **CIE L\\*a\\*b\\*:** The L\\* channel often has a range of $\\left[0, 100\\right]$ while the a\\* and b\\* channels are in the range $\\left[-100, +100\\right]$. In some cases, the data may be represented by signed 8-bit integers, in which case the range of values is acutally maped to the discrete space from $\\left[-128, +127\\right]$, so you may need to examine if your data is integer or float."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_FeL1EP5MfZF",
        "colab": {}
      },
      "source": [
        "# In this notebook, we are only considering greyscale and RGB data, so we\n",
        "# normalize by dividing by 255. If you transform the images into a different \n",
        "# colorspace, you will need to edit this code to properly normalize the data.\n",
        "if images[0,::].max() > 1:\n",
        "    images = images.astype(np.float32)/255.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5rNnXfmQwRB",
        "colab_type": "text"
      },
      "source": [
        "### Include an Axis for Color Channels\n",
        "In the case where we are operating on grayscale data, there should be an axis at the end of the array of length 1 to match up with TensorFlow expectations. Otherwise, you may get an error about not having the right number of dimensions for input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v912cBGrR4D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ed13d832-7192-4ab0-cf6e-7dd1f8a6addd"
      },
      "source": [
        "# Take note of number of color channels in the loaded image add a last axis to \n",
        "# images ndarray if array dimension is only 3 (as is the case with grayscale images)\n",
        "if images.ndim == 3:\n",
        "    n_channels = 1\n",
        "    # If image is grayscale, then we add a last axis (of len 1) for channel\n",
        "    images = images[:, : , :, np.newaxis]\n",
        "    print('\\nlast dimension added to images ndarray to account for channel')\n",
        "    print(f'new images.shape: {images.shape}')\n",
        "else:\n",
        "    #if image is not grayscale, last dimension of image already corresponds to channel\n",
        "    n_channels = images.shape[-1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "last dimension added to images ndarray to account for channel\n",
            "new images.shape: (5000, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIgUR8ckRtUO",
        "colab_type": "text"
      },
      "source": [
        "## Split the Image and Label Data\n",
        "Now we can split our pre-processed data into a training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EnTT4tyXMfY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "37e74dda-6c3e-4a54-fd06-d2f0df7a8018"
      },
      "source": [
        "# Split into training and testing sets\n",
        "tmp = train_test_split(images, labels, test_size = 0.2)\n",
        "train_images, test_images, train_labels, test_labels = tmp\n",
        "\n",
        "# Convert the labels from 1-D arrays to categorical type (one-hot encoding)\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Print sizes of train/test sets\n",
        "print(f'train_images.shape: {train_images.shape}')\n",
        "print(f'train_labels.shape: {train_labels.shape}')\n",
        "print(f'test_images.shape: {test_images.shape}')\n",
        "print(f'test_labels.shape: {test_labels.shape}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_images.shape: (4000, 48, 48, 1)\n",
            "train_labels.shape: (4000, 8)\n",
            "test_images.shape: (1000, 48, 48, 1)\n",
            "test_labels.shape: (1000, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QAgbBOzBMfZI"
      },
      "source": [
        "## Design Your CNN\n",
        "At this point, the data has been split into training and testing sets and normalized. We will now design a fully connected neural network for texture classification. \n",
        "\n",
        "\n",
        "![Example of a CNN Diagram](https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week3/CNN-example-block-diagram.jpg)\n",
        "\n",
        "\n",
        "(Image originally from http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/ )\n",
        "\n",
        "When designing a fully connected network for classification, we have several decisions to make.\n",
        "\n",
        "**Network Architecuture**\n",
        "* How many layers will our network have ?\n",
        "* How many convolutional filters per layer ?\n",
        "    * What is an appropriate filter size ? \n",
        "* What is an appropriate batch size, learning rate and number of training epochs ?\n",
        "\n",
        "**Data input**\n",
        "* Do we use the raw data ?\n",
        "    * RGB or just gray channel ?\n",
        "* Does the use of different colorspaces lead to better results for a given network architecture ?\n",
        "* Can we use any of the texture features from the previous lab as inputs to this model ?\n",
        "* How does data augmentation affect the results ? \n",
        "\n",
        "Other considerations, we will not be exploring :\n",
        "* What is the trade-off between input data sizes and batch size ?\n",
        "* Is the GPU always the appropriate platform for training ?\n",
        "* How does hardware influence inputs and batch sizes for a given desired accuracy ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OsffMC7mMfZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "7f526405-80aa-4118-e4cc-c82b5da26515"
      },
      "source": [
        "# Create your network\n",
        "\n",
        "# Build our model sequentially\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Add input layer\n",
        "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=8,\n",
        "                                 padding='valid',\n",
        "                                 activation=tf.nn.relu, \n",
        "                                 input_shape=(train_images.shape[1:4])))\n",
        "\n",
        "#use 2D max pooling to reduce size of convolution output before flattening\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(10,10),\n",
        "                                       strides=3, \n",
        "                                       padding='valid',\n",
        "                                       data_format='channels_last'))\n",
        "\n",
        "#Take all activations from previous layer and flatten them \n",
        "#(often done before a fully connected layer)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Add fully connected layers \n",
        "model.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n",
        "\n",
        "# Add final output layer - This should have as many neurons as the number\n",
        "# of classes we are trying to identify\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax)) \n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 41, 41, 16)        1040      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1936)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                61984     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 264       \n",
            "=================================================================\n",
            "Total params: 63,288\n",
            "Trainable params: 63,288\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DwOWHAVMfZU"
      },
      "source": [
        "## Compile and Train the Model\n",
        "\n",
        "Specify the loss function and optimization routine. Then compile the model you designed. Compiltation of the Keras model initializes the model weights and sets some other model properties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "azIn2h-PMfZV",
        "colab": {}
      },
      "source": [
        "# Specify the loss function to use\n",
        "loss_func = tf.keras.losses.categorical_crossentropy\n",
        "\n",
        "# Use the \"Adam\" adaptive learning algorithm to optimize the filter weights\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compile the model using the specified loss function and potimizer\n",
        "model.compile(loss=loss_func, optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffPMiOYRwzgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "130f65ff-4875-4399-c0dd-ec808dcf1b29"
      },
      "source": [
        "# Train Model\n",
        "\n",
        "#This function is called after each epoch\n",
        "#(It will ensure that your training process does not consume all available RAM)\n",
        "class garbage_collect_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n",
        "\n",
        "history = model.fit(train_images,   # Train examples\n",
        "          train_labels,             # Train labels\n",
        "          epochs=10,                # number of epochs\n",
        "          batch_size= 200,          # number of images for each iteration\n",
        "          callbacks=[garbage_collect_callback()],\n",
        "          validation_data=(test_images, test_labels), # Data for validation\n",
        "          verbose=True)             # Print info about optimization process\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 7s 334ms/step - loss: 2.0912 - accuracy: 0.1435 - val_loss: 2.0479 - val_accuracy: 0.1620\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 6s 325ms/step - loss: 2.0238 - accuracy: 0.1810 - val_loss: 2.0061 - val_accuracy: 0.1530\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 7s 330ms/step - loss: 1.9777 - accuracy: 0.1612 - val_loss: 1.9656 - val_accuracy: 0.1530\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 7s 327ms/step - loss: 1.9341 - accuracy: 0.1775 - val_loss: 1.9187 - val_accuracy: 0.1820\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 7s 329ms/step - loss: 1.8833 - accuracy: 0.1950 - val_loss: 1.8658 - val_accuracy: 0.2090\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 7s 327ms/step - loss: 1.8311 - accuracy: 0.2257 - val_loss: 1.8140 - val_accuracy: 0.2240\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 7s 332ms/step - loss: 1.7807 - accuracy: 0.2450 - val_loss: 1.7680 - val_accuracy: 0.2310\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 7s 328ms/step - loss: 1.7335 - accuracy: 0.2545 - val_loss: 1.7242 - val_accuracy: 0.2420\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 7s 327ms/step - loss: 1.6895 - accuracy: 0.2652 - val_loss: 1.6799 - val_accuracy: 0.2480\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 7s 332ms/step - loss: 1.6474 - accuracy: 0.2690 - val_loss: 1.6396 - val_accuracy: 0.2460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF1ufZSGeZ5m",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "Now that the model has been trained, we can use it to generate predictions and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ngFO-dYMfZa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7225d5d7-9c05-4808-ab04-b848ce63f690"
      },
      "source": [
        "# Evaluate model using test_images\n",
        "# WARNING: the value returned by model.evaluate depends on what metrics were\n",
        "# given to the model at compile time. Thus, if the metric is no longer accuracy,\n",
        "# the label \"model accuracy\" below will no longer be accurate\n",
        "\n",
        "test_binary_pred = model.predict(test_images)\n",
        "scores = model.evaluate(test_images, test_labels, verbose=False)\n",
        "print('Testing model on test set:')\n",
        "print(f'Model Loss: {scores[0]:.3f}, Model Accuracy: {scores[1]:.3f}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model on test set:\n",
            "Model Loss: 1.640, Model Accuracy: 0.246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sjAqutu2MfZf"
      },
      "source": [
        "Let's take a look at the confusion matrix for these model predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXGdzQ25e7gz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "dbc70b2d-fb7c-4c18-9590-d0bc8269289f"
      },
      "source": [
        "# First, convert the one-hot encoded true labels and predictied labels back into\n",
        "# a numerical classification value. This can be accomplished with the `argmax`\n",
        "# function and that neat `axis` keyword, again.\n",
        "test_true_labels = test_labels.argmax(axis=1)\n",
        "test_pred_labels = test_binary_pred.argmax(axis=1)\n",
        "\n",
        "# Generate the confusion matrix using these labels\n",
        "mat = confusion_matrix(test_true_labels, test_pred_labels)\n",
        "print(mat)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 38  35  50   0   0   0   0   0]\n",
            " [  7  55  22   1   2   0  33   0]\n",
            " [ 57  26  44   1   2   0   5   0]\n",
            " [  3  48   7   5   1   0  66   0]\n",
            " [ 10  62  14   0   1   0  42   0]\n",
            " [125   0   0   0   0   0   0   0]\n",
            " [  1  11   0   0   0   0 103   0]\n",
            " [121   0   2   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS3XfFfiftub",
        "colab_type": "text"
      },
      "source": [
        "To get a more visual representation of the matrix, let's plot it in matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h7RYbDzyMfZm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "38166733-3402-4bf0-cd2a-d02b9bbfe622"
      },
      "source": [
        "# Generate a new figure\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "# Display the confusion matrix\n",
        "plt.imshow(mat, cmap='hot', interpolation='nearest')\n",
        "\n",
        "# Add some anotation for the plot\n",
        "plt.colorbar()\n",
        "plt.xlabel('True label')\n",
        "plt.ylabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}