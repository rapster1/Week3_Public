{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "03_TransferLearning_Exercises.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PYOnt_yJkaI",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning Exercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNCmsEpLJkaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import useful libraries        (note: don't forget to turn on GPU)\n",
        "\n",
        "# tensorflow for network building/training\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Model, Sequential\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Basic operating system (os), numerical, and plotting functionality\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from matplotlib import pylab as plt\n",
        "\n",
        "# scikit-learn data utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage import transform\n",
        "\n",
        "# scikit-learn performance metric utilities\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Color transformations\n",
        "from skimage.color import rgb2lab\n",
        "\n",
        "#Skimage resizing \n",
        "from skimage.transform import resize\n",
        "\n",
        "# Garbage collection (for saving RAM during training)\n",
        "import gc"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IkUE60WJkaQ",
        "colab_type": "text"
      },
      "source": [
        "## VGG16 Model\n",
        "\n",
        "For this exercise you'll now use the VGG16 model as the feature extractor. https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16\n",
        "\n",
        "Specifications:\n",
        "- Default input size: 224x224, no smaller than 32x32 pixels\n",
        "- Default output classes: 1000\n",
        "\n",
        "Our images are 150x150 pixels in size and come from only **eight categories**. In order to use this model for our classification task, we again can/need to do the following:\n",
        "* Resize images : Our input images can be resized to the appropriate dimensions. Alternatively, we can pad our images to the expected dimensions. Padding leads to additional choices - Do we pad with zeros, duplicate edge pixels or mirror the image across edges?\n",
        "* Change the prediction layer : Remove the existing prediction layer and add a new layer that can predict **8 classes**.\n",
        "* Train : Finally, we need to train the network on our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teogiXA3Jkak",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwwkYk48Jkal",
        "colab_type": "text"
      },
      "source": [
        "Getting path and changing directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYdSaY2PJkam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the current directory and the directory where the files to download can\n",
        "# be found\n",
        "current_dir = os.getcwd()\n",
        "remote_path = 'https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/'\n",
        "\n",
        "# Define and build a directory to save this data in\n",
        "data_dir = os.path.join(current_dir, 'crc_data')\n",
        "if not os.path.isdir(data_dir):\n",
        "  os.mkdir(data_dir)\n",
        "\n",
        "# Move into the data directory and download all of the files\n",
        "os.chdir(data_dir)\n",
        "for ii in range(1, 6):\n",
        "    basename = f'rgb0{ii}.npz'\n",
        "    filename = os.path.join(remote_path, basename)\n",
        "\n",
        "    # Check if the file has already been downloaded\n",
        "    if not os.path.isfile(basename):\n",
        "      cmd = f'wget {filename}'\n",
        "      print(cmd)\n",
        "      os.system(cmd)\n",
        "\n",
        "# Return to the original directory\n",
        "os.chdir(current_dir)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoYrP7JXJkat",
        "colab_type": "text"
      },
      "source": [
        "Function for loading images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sesRgeVJkau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to load the data from the assumed download path\n",
        "def load_images(colorspace='rgb'):\n",
        "    \"\"\"\n",
        "    Loads the example data and applies transformation into requested colorspace\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    colorspace : str, optional, default: `rgb`\n",
        "        The colorspace into which the images should be transformed. Accepted\n",
        "        values include\n",
        "\n",
        "        'rgb' : Standard red-green-blue color-space for digital images\n",
        "\n",
        "        'gray' or 'grey': An arithmetic average of the (r, g, b) values\n",
        "\n",
        "        'lab': The CIE L*a*b* colorspace\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    images : numpy.ndarray, shape (Nimg, Ny, Nx, Ncolor)\n",
        "        The complete set of transformed images\n",
        "\n",
        "    labels : numpy.ndarray, shape (Nimg)\n",
        "        The classification labels associated with each entry in `images`\n",
        "\n",
        "    label_to_str : dict\n",
        "        A dictionary which converts the numerical classification value in\n",
        "        `labels` into its string equivalent representation.\n",
        "    \"\"\"\n",
        "    # Check that the colorspace argument is recognized\n",
        "    colorspace_lower = colorspace.lower()\n",
        "    if colorspace_lower not in ['rgb', 'gray', 'grey', 'lab']:\n",
        "        raise ValueError(f'`colorspace` value of {colorspace} not recognized')\n",
        "\n",
        "    # Load data, which is stored as a numpy archive file (.npz)\n",
        "    filename = os.path.join(data_dir, 'rgb01.npz')\n",
        "    print(f'loading {filename}')\n",
        "    tmp = np.load(os.path.join(data_dir, 'rgb01.npz'), allow_pickle=True)\n",
        "\n",
        "    # Parse the loaded data into images and labels\n",
        "    # Initialize the images and labels variables using the first archive data\n",
        "    images = tmp['rgb_data']\n",
        "    if colorspace_lower == 'rgb':\n",
        "        pass\n",
        "    elif colorspace_lower in ['gray', 'grey']:\n",
        "        images = np.mean(images, axis=-1)      # Average into grayscale\n",
        "    elif colorspace_lower == 'lab':\n",
        "        images = rgb2lab(images)               # Convert to CIE L*a*b*\n",
        "\n",
        "    # Grab the initial array for the image labels\n",
        "    labels = tmp['labels']\n",
        "    \n",
        "    # Grab the dictionary to convert numerical labels to their string equivalent\n",
        "    label_to_str = tmp['label_str']\n",
        "    label_to_str = label_to_str.tolist() # Convert label_to_str into a dict\n",
        "\n",
        "    # Update the user on the number and size of images loaded\n",
        "    print('Loaded images with shape {}'.format(images.shape))\n",
        "    del tmp\n",
        "\n",
        "    # Loop over each of the remaining archives and append the contained data\n",
        "    for ii in range(2,6):\n",
        "        # Build the full path to the archive and load it into memory\n",
        "        filename = os.path.join(data_dir, f'rgb0{ii}.npz')\n",
        "        print(f'loading {filename}')\n",
        "        tmp = np.load(filename, allow_pickle=True)\n",
        "\n",
        "        # Parse and append the data\n",
        "        these_images = tmp['rgb_data']\n",
        "        if colorspace_lower == 'rgb':\n",
        "            pass\n",
        "        elif (colorspace_lower == 'gray') or (colorspace_lower == 'grey'):\n",
        "            these_images = np.mean(these_images, axis=-1) # Convert to grayscale\n",
        "        elif colorspace_lower == 'lab':\n",
        "            these_images = rgb2lab(these_images)          # Convert to CIEL*a*b*\n",
        "\n",
        "        # Append the images and labels\n",
        "        images = np.append(images, these_images, axis=0)\n",
        "        labels = np.append(labels, tmp['labels'], axis=0)\n",
        "\n",
        "        # Update the user on the number and size of images\n",
        "        print('Loaded images with shape {}'.format(these_images.shape))\n",
        "        del tmp\n",
        "\n",
        "    # Force the image data to be floating point and print the data shape\n",
        "    images = images.astype(np.float)\n",
        "    print('Final image data shape: {}'.format(images.shape))\n",
        "    print('Number of image labels: {}'.format(*labels.shape))\n",
        "\n",
        "    return images, labels, label_to_str"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gg4B0EOJka1",
        "colab_type": "text"
      },
      "source": [
        "Load images and labels into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXzdtPccJka3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "27f19bc5-d033-4260-cf8e-a58a8c8eba03"
      },
      "source": [
        "images_full_res, labels, label_to_str = load_images()\n",
        "num_classes = np.unique(labels).size"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading /content/crc_data/rgb01.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "loading /content/crc_data/rgb02.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "loading /content/crc_data/rgb03.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "loading /content/crc_data/rgb04.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "loading /content/crc_data/rgb05.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "Final image data shape: (5000, 150, 150, 3)\n",
            "Number of image labels: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ri4n4KK3Ql",
        "colab_type": "text"
      },
      "source": [
        "## Pre-process the Images\n",
        "\n",
        "***Note: you'll have to edit a line of code in the cell for resizing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZqvdtsqJka7",
        "colab_type": "text"
      },
      "source": [
        "Resizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gAiiGUSJka7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "00644db4-31ea-4b2d-bf6f-3e669976bebc"
      },
      "source": [
        "# This boolean can be switched to false if you do not want to resize the images\n",
        "resize_images_bool = True\n",
        "\n",
        "# Specify a new shape to use for the resized images\n",
        "# NOTE: For the VGG16 model, we must use a size of at least (32, 32).\n",
        "original_shape = images_full_res.shape\n",
        "new_shape = list(original_shape)\n",
        "new_shape[1:3] = (64,64)\n",
        "\n",
        "# Compute if we are downsampling (in which case we need anti-aliasing)\n",
        "scaling_ratio = np.array(new_shape[1:3])/np.array(original_shape[1:3])\n",
        "anti_alias = np.any(scaling_ratio < 1)\n",
        "\n",
        "# If resizing is requested, then run the resizing transformation\n",
        "if resize_images_bool:\n",
        "    # Grab the original shape of the images\n",
        "    num_images = images_full_res.shape[0]\n",
        "\n",
        "    # Initialize an array for storing the resized images\n",
        "    images = np.zeros(new_shape, dtype=np.float16)\n",
        "\n",
        "    # Loop over each image in the data and perform a resizing operation\n",
        "    for img_num, img_data in enumerate(images_full_res):\n",
        "        # Update the user on progress\n",
        "        if np.mod(img_num, 1000) == 0:\n",
        "            print(f'Processing image number {img_num}')\n",
        "\n",
        "        # Process the image and force it to be a 16-bit float\n",
        "        processed_img = transform.resize(img_data, new_shape[1:],\n",
        "                                         anti_aliasing=anti_alias)\n",
        "        images[img_num] = processed_img.astype(np.float16)\n",
        "\n",
        "# If no resizing requested, then just rename that data\n",
        "else:\n",
        "    images = images_full_res\n",
        "\n",
        "# Remove the full-resolution versions from memory (just clogging things up)\n",
        "del images_full_res"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing image number 0\n",
            "Processing image number 1000\n",
            "Processing image number 2000\n",
            "Processing image number 3000\n",
            "Processing image number 4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQjuP1H1JkbE",
        "colab_type": "text"
      },
      "source": [
        "Normalize the images (if it hasn't been done already)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE3wO4fKJkbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note, we cast image data as float16 to save RAM\n",
        "images = images.astype(np.float16)/255.0"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA8WJ25FTKjS",
        "colab_type": "text"
      },
      "source": [
        "Include an axis for color channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNYIGee0TNK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take note of number of color channels in the loaded image add a last axis to \n",
        "# images ndarray if array dimension is only 3 (as is the case with grayscale images)\n",
        "if images.ndim == 3:\n",
        "    # If image is grayscale, then we add a last axis (of len 1) for channel\n",
        "    n_channels = 1\n",
        "    images = images[:, : , :, np.newaxis]\n",
        "    print('\\nlast dimension added to images ndarray to account for channel')\n",
        "    print(f'new images.shape: {images.shape}')\n",
        "else:\n",
        "    #if image is not grayscale, last dimension of image already corresponds to channel\n",
        "    n_channels = images.shape[-1]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHV6NK27JkbA",
        "colab_type": "text"
      },
      "source": [
        "Split data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwmn7EuuJkbA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "ed228a4d-d51c-4064-81fc-a6b6ea1ab60e"
      },
      "source": [
        "# Split data into train and test sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=.2)\n",
        "\n",
        "# Convert 'labels' (1D array of integers) to one-hot encoding\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Print sizes of train/test sets\n",
        "print(f'train_images.shape: {train_images.shape}')\n",
        "print(f'train_labels.shape: {train_labels.shape}')\n",
        "print(f'test_images.shape: {test_images.shape}')\n",
        "print(f'test_labels.shape: {test_labels.shape}')\n",
        "\n",
        "# Print the one-hot encoded labels as a sanity check\n",
        "print('one-hot encoded labels:')\n",
        "print(train_labels)\n",
        "\n",
        "# Get rid of the duplicate copies of the data\n",
        "del images, labels"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_images.shape: (4000, 64, 64, 3)\n",
            "train_labels.shape: (4000, 8)\n",
            "test_images.shape: (1000, 64, 64, 3)\n",
            "test_labels.shape: (1000, 8)\n",
            "one-hot encoded labels:\n",
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D9uhLlSJkbM",
        "colab_type": "text"
      },
      "source": [
        "## Load Pre-trained VGG16 Model\n",
        "\n",
        "here's the link to documentation again (https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16), also reference the tutorial notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om5GOu21JkbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e2ed1eee-6900-4bbb-92dd-7c6ac7fa43c6"
      },
      "source": [
        "# Create the base pre-trained model\n",
        "print('loading VGG16')\n",
        "base_model = tf.keras.applications.VGG16(input_shape=train_images.shape[1:], include_top=False, weights='imagenet')\n",
        "print('done')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading VGG16\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhA1SR6VJkbR",
        "colab_type": "text"
      },
      "source": [
        "Summarize model structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sQGrHQjJkbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "outputId": "099e4e01-be49-4163-b911-bedfb7600f28"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYNFl2ZOJkbZ",
        "colab_type": "text"
      },
      "source": [
        "Freezing layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLC3uT3cJkba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Play around with freezing layers, take a look at the tutorial notebook for reference \n",
        "\n",
        "# By default we'll just freeze the entire base model again\n",
        "base_model.trainable = False"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjn-7TTxJkbV",
        "colab_type": "text"
      },
      "source": [
        "Modify the pre-trained network by adding a few new layers at the output, including a classification layer (remember we want to predict 8 different classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-skRWsOAJkbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "121db132-3057-4a23-8d87-23278abbdc5f"
      },
      "source": [
        "# Add a global spatial average pooling layer\n",
        "## YOUR CODE HERE\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "# Add a fully-connected layer\n",
        "## YOUR CODE HERE\n",
        "prediction_layer1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "\n",
        "# Add the final classification layer\n",
        "## YOUR CODE HERE\n",
        "prediction_layer2 = tf.keras.layers.Dense(8, activation='softmax')\n",
        "\n",
        "# Build the model you will train\n",
        "## YOUR CODE HERE\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    prediction_layer1,\n",
        "    prediction_layer2\n",
        "])\n",
        "\n",
        "# Print summary of model layers\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 2, 2, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 14,981,448\n",
            "Trainable params: 266,760\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvwvRkiVJkbd",
        "colab_type": "text"
      },
      "source": [
        "Compiling model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-_ZFi_Jkbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model (should be done *after* setting layers to non-trainable)\n",
        "    # optimizer: rmsprop\n",
        "    # loss: categorical crossentropy\n",
        "    # metrics: accuracy\n",
        "  \n",
        "## YOUR CODE HERE\n",
        "base_learning_rate = 0.0001\n",
        "opt = tf.keras.optimizers.RMSprop(lr=base_learning_rate)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29DxiSHbJkbh",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsYo9vGeeMBM",
        "colab_type": "text"
      },
      "source": [
        "Train the model on the new, histological, data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXtYUBTOeWl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our training and validation ('test') data to TensorFlow data\n",
        "# This prevents the training algorithm from needing to make a *copy* of your\n",
        "# numpy arrays, which would EAT UP SOO MUCH RAM!\n",
        "#\n",
        "# It also accelerates training a bit because there is no data-conversion step\n",
        "train_images_tf = tf.constant(train_images, dtype=tf.float16)\n",
        "test_images_tf = tf.constant(test_images)\n",
        "del train_images, test_images\n",
        "\n",
        "train_labels_tf = tf.constant(train_labels, dtype=tf.float16)\n",
        "test_labels_tf = tf.constant(test_labels)\n",
        "del train_labels, test_labels"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J-uBwdWerN5",
        "colab_type": "text"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpKK3afvJkbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "31bda21d-efa4-44f5-e704-a0539b36633c"
      },
      "source": [
        "# This function is called after each epoch\n",
        "# (It will ensure that your training process does not consume all available RAM)\n",
        "class garbage_collect_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n",
        "\n",
        "# Time how long it takes the model to train for these epochs\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform the training method\n",
        "history = model.fit(train_images_tf,\n",
        "                    train_labels_tf,\n",
        "                    batch_size=64,\n",
        "                    epochs= 10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(test_images_tf, test_labels_tf),\n",
        "                    callbacks = [garbage_collect_callback()])\n",
        "\n",
        "stop_time = time.time()\n",
        "print(\"--- %s seconds ---\" % (stop_time - start_time))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 183s 3s/step - loss: 0.4297 - accuracy: 0.8493 - val_loss: 0.5743 - val_accuracy: 0.7910\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 185s 3s/step - loss: 0.3897 - accuracy: 0.8670 - val_loss: 0.5054 - val_accuracy: 0.8190\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 183s 3s/step - loss: 0.3504 - accuracy: 0.8765 - val_loss: 0.4155 - val_accuracy: 0.8470\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8850"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-1bf0b59379c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     callbacks = [garbage_collect_callback()])\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mstop_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    870\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m    873\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPGyseHcJkbk",
        "colab_type": "text"
      },
      "source": [
        "Plot model train/validation accuracy and model train/validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXegpORpJkbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8GR-I4mJkbp",
        "colab_type": "text"
      },
      "source": [
        "## Make Predictions for Test Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QucAFGw1Jkbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict class of test each test\n",
        "predictions = model.predict(test_images_tf, verbose=True)\n",
        "\n",
        "# Convert the predictions and true labels into category numbers\n",
        "test_true_labels = test_labels_tf.numpy().argmax(axis=1)\n",
        "test_pred_labels = predictions.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHx3kt6GJkbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot a set of test images, along with predicted labels and true labels\n",
        "plt.figure(figsize=(16,20))\n",
        "for ii in range(0, 16):\n",
        "    # Activate subplot and display image\n",
        "    plt.subplot(4,4,ii+1)\n",
        "    plt.imshow(test_images_tf[ii+100,:,:,:].numpy().astype(np.float))\n",
        "\n",
        "    # Turn off axes\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add annotaiton\n",
        "    plt.title('expected : ' + label_to_str[test_true_labels[ii+100]]\n",
        "              + '\\npredicted : ' + label_to_str[test_pred_labels[ii+100]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntDR6ZWYiEgN",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRrEEXx9iGZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = accuracy_score(test_true_labels, test_pred_labels)\n",
        "print(f'Model Accuracy: {acc:.2%}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpj8PtZuiIAU",
        "colab_type": "text"
      },
      "source": [
        "Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp7rdgkciJJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_mat = confusion_matrix(test_true_labels, test_pred_labels)\n",
        "\n",
        "# Generate a new figure\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "# Display the confusion matrix\n",
        "plt.imshow(conf_mat, cmap='hot', interpolation='nearest')\n",
        "\n",
        "# Add some anotation for the plot\n",
        "plt.colorbar()\n",
        "plt.xlabel('True label')\n",
        "plt.ylabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFwpTudBiMdv",
        "colab_type": "text"
      },
      "source": [
        "### To-do:\n",
        "\n",
        "Continue playing around with preprocessing (image size) and the model (added layers, freezing layers, optimizer, # epochs) and see their effects on the accuracy. Doing this may help you for the challenge problem :O"
      ]
    }
  ]
}